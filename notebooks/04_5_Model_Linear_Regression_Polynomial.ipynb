{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_5 Linear Regression with Polynomial Features\n",
    "\n",
    "Due to NDA agreements no data can be displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation, Data Cleaning, and Preparation for Modelling is the same for all algorithms. To directly go to modelling click [here](#modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import mlflow\n",
    "from modeling.config import EXPERIMENT_NAME\n",
    "TRACKING_URI = open(\"../.mlflow_uri\").read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('../data/Featureselection03.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data frame with important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that everyone is on track with the feature selection, we created another csv file to rate the importance and only use important features for training our models and further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only important features are used to train the model. In this case we use 17 features beside the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read list with feature importance\n",
    "data_log = pd.read_csv('../data/Capstone_features_Features.csv')\n",
    "data_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of important features (feature importance < 3)\n",
    "list_imp_feat = list(data_log[data_log['ModelImportance'] < 3]['VarName'])\n",
    "len(list_imp_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df[list_imp_feat].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill and drop NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for V.SLPOG.act.PRC and ME.SFCI.act.gPkWh contain missing values. The EDA showed that these are mainly caused during harbour times when the main engine was not running. Therefore it makes sense to fill the missing values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model['V.SLPOG.act.PRC'].fillna(0,inplace=True)\n",
    "df_model['ME.SFCI.act.gPkWh'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining rows with missing values are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During further analysis one sample with an ME.FMS.act.tPh of 8 was defined as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model[df_model['ME.FMS.act.tPh'] < 8]\n",
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,28))\n",
    "sns.heatmap(df_model.corr(), annot = True, cmap = 'RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V.SOG.act.kn is still highly correlated with the target, but this feature is necessary to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project the focus is on optimising the fuel consumption. Therefore the supply mass rate is used as target. Target values greater 8 t/h are defined as outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model.drop(['ME.FMS.act.tPh'], axis = 1) #, 'Power_EM_predict', 'ME.SFCI.act.gPkWh'\n",
    "y = df_model['ME.FMS.act.tPh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the high amount of data, a split into 10% test data and 90% train data is chosen. The random state is set to 42 to have comparable results for diffent models. To account for the imbalance in the distribution of passage types the stratify parameter is used for this feature. This results in approximately the same percentage of the different passage types in each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = X['passage_type'], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy values for passage type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object types need to be transformed to dummy values. For this model this concerns the passage types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set MLFlow connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow is used to track and compare different models and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.start_run(run_name='LinReg_with_PolyFeatures_no_EM_SFCI_>13.5kn') # CHANGE!\n",
    "run = mlflow.active_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the modeling a SGD Linear Regression was used in this notebook. Polynomial features of the second poylonmial were included as well as all polynomial features. This was done to be able to model higher polynomials and to include conections between the features.\n",
    "\n",
    "For all models in this project a MinMaxScaler is applied. For this model a random forrest is used. The hyperparameter are selected based on grid search and offer a reasonable balance between optimal results and overfitting. These settings are used in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = make_pipeline(PolynomialFeatures(degree=2), MinMaxScaler() , LinearRegression()) # CHANGE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "y_pred_train = reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model also negative values for the target are being predicted. This does not make sense. Therefore these values were set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred < 0] = 0\n",
    "y_pred_train[y_pred_train < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors and residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root mean squared error (RMSE) is used to evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE train: ', mean_squared_error(y_train, y_pred_train, squared= False))\n",
    "rmse_train = mean_squared_error(y_train, y_pred_train, squared= False)\n",
    "print('RMSE test: ', mean_squared_error(y_test, y_pred, squared= False))\n",
    "rmse_test = mean_squared_error(y_test, y_pred, squared= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting actual values against predicted shows that the points are close to the optimal diagonale. However, this plot and the yellowbrick residual plot show some dificulties the model has when predicting low target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(6, 6))\n",
    "plt.axline([1, 1], [2, 2],color='lightgrey')\n",
    "plt.scatter(y_train, y_pred_train, color ='#33424F')\n",
    "plt.scatter(y_test, y_pred, color = '#FF6600')\n",
    "#plt.xticks(np.arange(0,501,100));\n",
    "#plt.yticks(np.arange(0,501,100));\n",
    "plt.xlabel(\"ME.FMS.act.tPh actual\");\n",
    "plt.ylabel(\"ME.FMS.act.tPh predicted\");\n",
    "plt.xlim(-1, 6);\n",
    "plt.ylim(-1, 6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it can be seen, that especially while trying to predict lower values for the target the model makes some errors in both directions. For higher values the model performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seting parameters that should be logged on MLFlow\n",
    "#these parameters were used in feature engineering (inputing missing values)\n",
    "#or parameters of the model (fit_intercept for Linear Regression model)\n",
    "params = {\n",
    "      \"features drop\": 'according to Capstone_features_Features.csv + t/h kleiner 8 + EM and SCFI drop + Speed > 13.5 kn',\n",
    "      \"explanation\": 'Linear Regression with Polynomial features of degree 2 and all negative Prediction set to 0',\n",
    "      \"csv used\": 'Featureselection03.csv',\n",
    "      \"NaN handling\": 'V.SLPOG.act.PRC and ME.SFCI.act.gPkWh filled with 0, rest dropped by row',\n",
    "      'Shape' : df.shape,\n",
    "      'Scaler' : 'MinMaxScaler',\n",
    "      'Polynomialdegree' : 2,\n",
    "      'size_test' : 0.2\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging params to mlflow\n",
    "mlflow.log_params(params)\n",
    "#setting tags\n",
    "mlflow.set_tag(\"running_from_jupyter\", \"True\")\n",
    "#logging metrics\n",
    "mlflow.log_metric(\"train-\" + \"RMSE\", rmse_train)\n",
    "mlflow.log_metric(\"test-\" + \"RMSE\", rmse_test)\n",
    "# logging the model to mlflow will not work without a AWS Connection setup.. too complex for now\n",
    "# but possible if running mlflow locally\n",
    "# mlflow.log_artifact(\"../models\")\n",
    "# mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nr. of input features: ', reg.steps[0][1].n_features_in_)\n",
    "print('Nr. of output features: ', reg.steps[0][1].n_output_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 features were input and due to the inclusion of polynomial features 153 were used for prediction. What are these features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.steps[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature '1' at the beginning is the constant that explains the y axis section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.steps[2][1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most important features for the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef = pd.DataFrame({'features' : reg.steps[0][1].get_feature_names_out(), 'coef' : reg.steps[2][1].coef_})\n",
    "df_coef.sort_values('coef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of the errors and understand the model better to be able to improve it further, an error analysis was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Residuals Plot from the yellowbrick package was used.\n",
    "\n",
    "In the Residual plot the predicted values are ploted against the Residuals (Errors) to identify patterns were the models makes more or larger errors compared to other areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(reg)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again it is observable that especially for smaller values larger errors are observable. The result where negative values were set to 0 can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_test = y_pred - y_test\n",
    "residuals_train = y_pred_train - y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = y_pred_train, y = residuals_train)\n",
    "sns.scatterplot(x = y_pred, y = residuals_test)\n",
    "plt.axhline(y = 0, color = 'black')\n",
    "plt.xlabel(\"ME.FMS.act.tPh predicted\");\n",
    "plt.ylabel(\"Residuals\");\n",
    "plt.legend(labels=['', 'train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the other plots it is observable that the model makes the most errors when predicting values that are actual 0 but the model predicts a higher value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding what happens when the t/h is 0 but values are getting predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse why the model has trouble with these values, these values were analysed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_actzeros = X_train.copy()\n",
    "X_train_actzeros['actual'] = y_train\n",
    "X_train_actzeros['predict'] = y_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data points that are actual 0 values but the model predicts with more than 0.1 (little below the RMSE) are problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_actzeros = X_train_actzeros[X_train_actzeros['actual'] == 0]\n",
    "X_train_actzeros = X_train_actzeros[X_train_actzeros['predict'] > 0.1]\n",
    "X_train_actzeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_actzeros.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=X_train_actzeros,\n",
    "                  x_vars=['predict'],\n",
    "                  y_vars=['ME.FTS.act.dgC', 'HFO.GME.act.nodim', 'V.COG.act.deg', 'V.SOG.act.kn',\n",
    "       'V.RUA.act.deg','WEA.WDT.act.deg', 'WEA.WST.act.mPs', 'V.SLPOG.act.PRC',\n",
    "        'DDM.TRIM.act.m', 'DDM.DRAFT.act.m',\n",
    "       'Wave_Height_m_daily', 'True_Wave_Dir_deg_daily'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most promising was the V.SOG.act.kn. Therefore it was analysed more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(X_train_actzeros, x='V.SOG.act.kn',\n",
    "            y='predict',\n",
    "            color='predict',\n",
    "            hover_data=['V.RUA.act.deg'],\n",
    "            labels={'x':'V.SOG.act.kn','y':'predict','color':'V.RUA.act.deg'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these datapoints are at a speed between 0 and 10 kn. Some of them could be explained by being towed by a tugboat. No data was given wether a tugboat was used. Therefore it is difficult to analyse further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing points with high residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res = X_train.copy()\n",
    "X_train_res['actual'] = y_train\n",
    "X_train_res['predict'] = y_pred_train\n",
    "X_train_res['residuals'] = residuals_train\n",
    "X_train_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datapoints with an error of +1 or -1 were analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res = X_train_res[(X_train_res['residuals'] > 1) | (X_train_res['residuals'] <  -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=X_train_res,\n",
    "                  x_vars=['predict'],\n",
    "                  y_vars=['ME.FTS.act.dgC', 'HFO.GME.act.nodim', 'V.COG.act.deg', 'V.SOG.act.kn',\n",
    "       'V.RUA.act.deg','WEA.WDT.act.deg', 'WEA.WST.act.mPs', 'V.SLPOG.act.PRC',\n",
    "        'DDM.TRIM.act.m', 'DDM.DRAFT.act.m',\n",
    "       'Wave_Height_m_daily', 'True_Wave_Dir_deg_daily'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most promising feature was again the V.SOG.act.kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(X_train_res, x='actual',\n",
    "            y='predict',\n",
    "            color='V.SOG.act.kn',\n",
    "            hover_data=['V.RUA.act.deg'],\n",
    "            labels={'x':'V.SOG.act.kn','y':'predict','color':'V.RUA.act.deg'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No possible pattern could be seen. The datapoints are distributed without a recognizable pattern in the color."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
