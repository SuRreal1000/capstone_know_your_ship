{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - Combine Database Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook processes raw information and provides at the end a `single CSV file`. This file contains 5 month of vessel-sensor-data, July 2021 until end of October 2021, including a timestamp in minute intervals and 110 other features.\n",
    "\n",
    "The original raw data was provided within single db-files (.s3db format), containing db-tables where two have been used in this notebook:\n",
    "* DataLogEntry: containing sensor data;\n",
    "* DataLog: containing high level information on the different features.\n",
    "\n",
    "For some features, the sensor data is available in 5 seconds and 10 seconds frequency. However for the majority the data is only available in 1 minute steps. In addition, for some timestamps multiple measurements are logged. In order to reduce sensor data to one measurement per minute, multiple measurements per minute (e.g. 5sec, 10sec, or multiple measurements per minute) the first value was taken.\n",
    "\n",
    "The dataset further contained column/feature, called EntryState. For EntryState = 0 all values were observed to be 0. It is assumed that for those Values are not available and were set to NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "  \n",
    "1) Copy original Files into repos data folder.\n",
    "\n",
    "2) If required, adjust folder/dbfiles information in `File setting` section below\n",
    "\n",
    "3) Run Main Code, CSVs will be saved to repos data folder\n",
    "    * STAGE_Bluetracker.csv: Combined db-files to one csv;\n",
    "    * FINAL_Bluetracker.csv: Adjusted Headings and Columns with only NaN values removed > 110 features remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only few Libraries required for this notebook. \n",
    "* sqlite3 for reading the db-files using SQL;\n",
    "* os to read filenames in folder;\n",
    "* `magic happens all in pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those tree functions are used for:\n",
    "* Combining the raw db-files (beginning of this notebook);\n",
    "* Header matching (see end of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List with File Names (Full Path) / Loop through given dictionary\n",
    "def _fGetFullPathFileList(directory_in_str, filetype=\".s3db\"):\n",
    "    '''\n",
    "    This function returns a list. The list includes the full path of files in a given directory that match >filetype< \n",
    "    '''\n",
    "    directory = os.fsencode(directory_in_str)\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(filetype): # or filename.endswith(\".py\"): \n",
    "            #print(f'{directory_in_str}/{filename}')\n",
    "            files.append(f'{directory_in_str}/{filename}')\n",
    "            #print(directory)\n",
    "            #print(os.path.join(directory, filename))\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    return files\n",
    "#print(_fGetFullPathFileList(FILEDIR, '.s3db')[0:5])\n",
    "\n",
    "# Retruns Db-Table as Dataframe\n",
    "def _fGetDBTableDataframe(full_dbpath_as_str, tablename_as_str):\n",
    "    '''\n",
    "    Returns a Dataframe with all Data from a given database and tablename. Requires 'import sqlite3' library\n",
    "    '''\n",
    "    try:\n",
    "        cnx = sqlite3.connect(full_dbpath_as_str)\n",
    "        cur = cnx.cursor()\n",
    "        #print(\"Database created and Successfully Connected to SQLite\")\n",
    "\n",
    "        # Read table to dataframe\n",
    "        path = f'SELECT * FROM {tablename_as_str}'\n",
    "        df = pd.read_sql_query(path, cnx)\n",
    "\n",
    "        cur.close()\n",
    "\n",
    "    except sqlite3.Error as error:\n",
    "        print(f\"{full_dbpath_as_str} >> Error while connecting to sqlite:\", error)\n",
    "    finally:\n",
    "        if cnx:\n",
    "            cnx.close()\n",
    "\n",
    "            #print(\"The SQLite connection is closed\")\n",
    "            return df.copy()\n",
    "\n",
    "# Combines DB-Tables to one DataFrame. It takes a list of DBfilenames and the tablename as input\n",
    "def _fCombinedTableDataframe(list_db_paths, table_name='Log_10s', status=False):\n",
    "    '''\n",
    "    Write single db tables into one dataframe. Calls _fGetDBTableDataframe\n",
    "    '''\n",
    "    print('-'*50)\n",
    "    print(f'Combining Tables: {table_name}')\n",
    "    i = 0\n",
    "    frames = []\n",
    "    for s3db in list_db_paths:\n",
    "        df = _fGetDBTableDataframe(s3db, table_name)\n",
    "        frames = pd.concat([pd.DataFrame(frames), df])\n",
    "        del df\n",
    "        # Just some stupid status bar :P\n",
    "        if status:\n",
    "            i += 1\n",
    "            l = len(list_db_paths)\n",
    "            if i % 6 == 0 or i == l:\n",
    "                print('|', '#' * i, ' ' * (l - i), '|', frames.shape)\n",
    "\n",
    "    print(f'Finished combining tables \"{table_name}\" | Shape: {frames.shape} ')\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `File Settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the Folder path where all our .s3db Files are. Further we define the Table name where the measurements are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are the db-files stored?\n",
    "FILEDIR = '../data/original'\n",
    "\n",
    "# Whats the type of the DB file?\n",
    "DBTYPE = '.s3db'\n",
    "\n",
    "# What is the relevant Tablename?\n",
    "TABLE_NAME = 'DataLogEntry'\n",
    "\n",
    "# Where would you like to save CSV files?\n",
    "CSVFOLDER = '../data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Get Raw data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the code to combine all our single Tables into one dataframe/csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(FILEDIR)\n",
    "    # Run the code to combine all DB Tables / Runtime for 6 month Bluetracker Data = 30ish minutes\n",
    "    df = _fCombinedTableDataframe(_fGetFullPathFileList(FILEDIR, DBTYPE), TABLE_NAME, True)\n",
    "\n",
    "    # Save the data to predefined folder\n",
    "    df.to_csv(f'{CSVFOLDER}/STAGE_Bluetracker.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative, if you already run the combine code above you can simply load the csv-file. This is way faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (39440874, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{CSVFOLDER}/STAGE_Bluetracker.csv')    # 30-40 seconds runtime\n",
    "print(f'Dataframe shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape after dropping EntryState==0: (38422797, 4)\n"
     ]
    }
   ],
   "source": [
    "# Delete EntryState = 0 Values\n",
    "df = df[df.EntryState==2].copy()\n",
    "df.drop('EntryState', inplace=True, axis=1)\n",
    "print(f'Dataframe shape after dropping EntryState==0: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of seconds for Entry Date\n",
    "df['EntryDate'] = df.EntryDate.apply(lambda row: str(row)[0:16])\n",
    "\n",
    "# Change EntryDate to datetime type\n",
    "df['EntryDate'] = pd.to_datetime(df['EntryDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty Dataframe\n",
    "df_dates = pd.DataFrame()\n",
    "\n",
    "# Create Date Values and rename Column\n",
    "df_dates['EntryDate'] = pd.date_range(start=df['EntryDate'].min(), end=df['EntryDate'].max(), freq='T')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape after dropping duplicates: (23380630, 4)\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['EntryDate', 'DataLogID'], keep='first', inplace=True)\n",
    "print(f'Dataframe shape after dropping duplicates: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='DataLogEntryID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_logid = list(df.DataLogID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape after dropping duplicates: (220321, 123)\n"
     ]
    }
   ],
   "source": [
    "# Write DataLogIDs to own column\n",
    "for LOGID in list_logid:\n",
    "    df_dates = pd.merge(df_dates, df[df.DataLogID == LOGID].drop(columns='DataLogID'), how='left', on='EntryDate')\n",
    "    df_dates.rename(columns={\"EntryValue\": LOGID}, inplace=True)\n",
    "df = df_dates.copy()\n",
    "del df_dates\n",
    "print(f'Dataframe shape after dropping duplicates: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EntryDate</th>\n",
       "      <th>11014</th>\n",
       "      <th>11011</th>\n",
       "      <th>11015</th>\n",
       "      <th>11020</th>\n",
       "      <th>11017</th>\n",
       "      <th>11021</th>\n",
       "      <th>14003</th>\n",
       "      <th>11012</th>\n",
       "      <th>11016</th>\n",
       "      <th>...</th>\n",
       "      <th>12070</th>\n",
       "      <th>12061</th>\n",
       "      <th>12062</th>\n",
       "      <th>14006</th>\n",
       "      <th>13004</th>\n",
       "      <th>14005</th>\n",
       "      <th>13002</th>\n",
       "      <th>13000</th>\n",
       "      <th>13003</th>\n",
       "      <th>13001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-31 00:00:00</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>1.622416e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-31 00:01:00</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>1.622416e+09</td>\n",
       "      <td>86402.0</td>\n",
       "      <td>1.343</td>\n",
       "      <td>...</td>\n",
       "      <td>192.031204</td>\n",
       "      <td>293.132843</td>\n",
       "      <td>290.477203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-31 00:02:00</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.622416e+09</td>\n",
       "      <td>86402.0</td>\n",
       "      <td>1.343</td>\n",
       "      <td>...</td>\n",
       "      <td>193.393600</td>\n",
       "      <td>291.190186</td>\n",
       "      <td>289.918213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-31 00:03:00</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>1.622416e+09</td>\n",
       "      <td>86402.0</td>\n",
       "      <td>1.343</td>\n",
       "      <td>...</td>\n",
       "      <td>194.051834</td>\n",
       "      <td>290.139618</td>\n",
       "      <td>291.215454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-31 00:04:00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>1.622416e+09</td>\n",
       "      <td>86402.0</td>\n",
       "      <td>1.343</td>\n",
       "      <td>...</td>\n",
       "      <td>195.484314</td>\n",
       "      <td>288.614624</td>\n",
       "      <td>285.923615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EntryDate  11014  11011  11015  11020  11017  11021         14003  \\\n",
       "0 2021-05-31 00:00:00   65.0    0.0 -0.004   28.0    0.0 -0.002  1.622416e+09   \n",
       "1 2021-05-31 00:01:00   58.0    0.0 -0.003   25.0    0.0 -0.002  1.622416e+09   \n",
       "2 2021-05-31 00:02:00   63.0    0.0  0.007   27.0    0.0  0.000  1.622416e+09   \n",
       "3 2021-05-31 00:03:00   63.0    0.0  0.008   27.0    0.0 -0.001  1.622416e+09   \n",
       "4 2021-05-31 00:04:00   64.0    0.0  0.000   27.0    0.0 -0.001  1.622416e+09   \n",
       "\n",
       "     11012  11016  ...       12070       12061       12062  14006  13004  \\\n",
       "0      NaN    NaN  ...         NaN         NaN         NaN    NaN    NaN   \n",
       "1  86402.0  1.343  ...  192.031204  293.132843  290.477203    NaN    NaN   \n",
       "2  86402.0  1.343  ...  193.393600  291.190186  289.918213    NaN    NaN   \n",
       "3  86402.0  1.343  ...  194.051834  290.139618  291.215454    NaN    NaN   \n",
       "4  86402.0  1.343  ...  195.484314  288.614624  285.923615    NaN    NaN   \n",
       "\n",
       "   14005  13002  13000  13003  13001  \n",
       "0    NaN    NaN    NaN    NaN    NaN  \n",
       "1    NaN    NaN    NaN    NaN    NaN  \n",
       "2    NaN    NaN    NaN    NaN    NaN  \n",
       "3    NaN    NaN    NaN    NaN    NaN  \n",
       "4    NaN    NaN    NaN    NaN    NaN  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Column /Feature Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about the Features/Sensors/Columns are stored in the db-files in a Table called `DataLog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some db file path + name\n",
    "files = _fGetFullPathFileList(FILEDIR, DBTYPE)\n",
    "print(files[0])\n",
    "\n",
    "# Read to dataframe\n",
    "df_heading = _fGetDBTableDataframe(files[0], 'DataLog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heading.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate VarNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, som VarNames appearing multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heading.VarName.value_counts().head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_varname = {13002: 'ME.LOD.act.PRC.trend', \\\n",
    "    13003: 'ME.SFC.act.gPkWh.trend', \\\n",
    "        13004: 'AE.SFC.act.gPkWh.trend', \\\n",
    "            13006: 'V.SLPTW.act.PRC.trend', \\\n",
    "                14005: 'V.STW.act.kn.trend'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heading[df_heading.VarName=='ME.LOD.act.PRC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will match now the DataLogID with the VarName and replace the column/feature names by `VarName`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns\n",
    "for LOGID in list_logid:\n",
    "    if LOGID in dic_varname:\n",
    "        VARNAME = dic_varname[LOGID]\n",
    "    else:\n",
    "        VARNAME = df_heading[df_heading.DataLogID == LOGID].VarName.unique()[0]\n",
    "    df.rename(columns={LOGID: VARNAME}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220321, 123)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready and can save our DataFrame to our FINAL_Bluetracker.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('EntryDate', inplace=True)\n",
    "df.to_csv(f'{CSVFOLDER}/FINAL_Bluetracker_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EntryDate                          0\n",
       "LM1.plc_cpuload.avg_5s.PRC      8642\n",
       "LM1.plc_skewtime.act.s          8642\n",
       "LM1.plc_timeBalance.act.s       8642\n",
       "LM2.plc_cpuload.avg_5s.PRC      8642\n",
       "                               ...  \n",
       "V.STW.act.kn.trend            220319\n",
       "ME.LOD.act.PRC.trend          220319\n",
       "ME.POW.act.MW                 220319\n",
       "ME.SFC.act.gPkWh.trend        220319\n",
       "ME.SPD.act.rpm                220319\n",
       "Length: 123, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad8f7a3d93ea9b275449e95c0ce6ee25301282232d0b608b79db5bcbac100c75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
