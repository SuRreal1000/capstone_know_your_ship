{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_2 Feature engineering\n",
    "Due to NDA agreements no data can be displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the DataFrame for the use in the Machine Learning models is generated and put together.  \n",
    "Therefore the high frequency data is sorted and the number of features reduced. Then the DataFrame is enriched with daily data from the noon report and with predicted data from the Engine Model.  \n",
    "On top the sensor data from the draft is flattened out with a rolling average and also included into the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection of high frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook the data from the notebook \"Preprocessing\" is used. The data contains 5 month of sensor-data (110 features plus timestamp) from July 2021 to October 2021. Measurements are logged in minute intervals. For details of preprocessing like e.g. handling of duplicates and identification of missing values, see notebook \"Preprocessing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('../data/FINAL_Bluetracker_mean_1m_ES0NaN.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have timestamps from {} to {}. Therefore a series ranging over 5 months.'.format(df.EntryDate.min(), df.EntryDate.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 111 features over a period of 5 month to look at. One of them is the timestamp. A detailed look on the feature names and explanations showed that some features might be more important than others. Less important might e.g. information about the controller or duplicate information that are given in different units or granularity. The following feature are considered to be not important:\n",
    "* Information about the Programable Logic Controller, i.e. feature names starting with ```LM1.plc_``` or ```LM2.plc_```\n",
    "    * This block of features give information about the computation load but not about the vessel performance. Thus, they are considered to be not relevant for this study.\n",
    "* Detailed information about the auxiliary engine (```AE{1-X}.LOD.act.PRC``` and ```AE{1-X}.POW.act.kW```)\n",
    "    * These information are given for each of the auxiliary engines separately and as totals. For this study, only the totals are considered.\n",
    "* Fuel oil temperature (```{engine}.FTS.act.dgC```)\n",
    "    * Heavy fuel oil needs to be heated before using it for combustion. This process is mandatory and cannot be adjusted. Therefore these features are dropped.\n",
    "* Rate of Turn (```V.ROT.act.degPmin```)\n",
    "    * This feature might be important for daily business, but has less effect on the general performance of the vessel\n",
    "* Vessel distance (```V.DOG.cnt_tot.nm``` and ```V.DTW.cnt_tot.NM```)\n",
    "    * For this study the actual location, course and speed are considered to be more important. The vessel distance is probably more of interest for the shipping company.\n",
    "* Total running information\n",
    "    * We are investigating only one vessel, thus there is no need and possibility to compare with other vessel and different running hours.\n",
    "* Heavy fuel oil consumption totals ```{engine}.{fuel type}.cnt_tot.t```\n",
    "    * We will work with the current fuel oil consumptions, thus the totals are not necessary. The same applies for the average fuel oil consumption (```{engine}.SFC.avg_tot.gPkWh```).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are stored in a list where feature importance has been added. This feature importance will be used to exclude features.\n",
    "In a scooring from 1-3 the importance has been set for each feature, where\n",
    "* 1: keep the feature\n",
    "* 2: might be importent\n",
    "* 3: drop the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read list with feature importance\n",
    "data_log = pd.read_csv('../data/Capstone_features_Features.csv')\n",
    "data_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of important features (feature importance < 3)\n",
    "list_imp_feat = list(data_log[data_log['F_Imp_new'] < 3]['VarName'])\n",
    "len(list_imp_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all features in the data frame\n",
    "columns = df.columns\n",
    "len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of important features includes some features the have no entry values in our data frame, i.e. the high frequency sensor data. Therefore we have to exclude these from our list of important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of important features that are not included in our data\n",
    "list_notinfeat = list(set(list_imp_feat) - set(columns))\n",
    "len(list_notinfeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some features that have the same VarName but different DataLogID and LogName. We figured out that these features are marked with ```trendlog```. The trendlog DataLogIDs do not have measurements in the delivered high frequency data. Therefore, we remove the duplicate feature names from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in list of important features\n",
    "list_double = []\n",
    "for i in list_imp_feat:\n",
    "    if list_imp_feat.count(i) > 1:\n",
    "        list_double.append(i)\n",
    "set(list_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "list_imp_feat = list(set(list_imp_feat))\n",
    "len(list_imp_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these information and the preparatory steps, a list with the features that can be dropped is created. Afterwards, these features are dropped from the data frame with the high frequency sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_imp_feat = list(set(list_imp_feat) - set(list_notinfeat))\n",
    "len(list_imp_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_todrop = list(set(columns) - set(list_imp_feat))\n",
    "len(list_todrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the features from the DateFrame\n",
    "df = df.drop(list_todrop, axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping 82 features that are considered to be not important, there are now 28 features plus the time stamp left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are floats, which makes sense, since we obtained sensor data. The following table gives an overview of the feature statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum().sum(), 'out of', df.shape[0]*df.shape[1], 'entries are NaN.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()/(df.shape[0]*df.shape[1])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.88% of the sensor measurements are missing values. It should be further investigated how to deal with these missing values. Do they make sense for at least some features? Are there single sensors that are prone to dropout? Is there a possibility to fill them reasonable? These questions will be further regarded in the EDA.  At this stage, the focus is still on the feature selection. In order to check if there are features that are related to each other and thus have no added value, a correlation matrix for all feature is evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining passage types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geoplot with location, speed and date to visualize the positions and speed of the vessel\n",
    "fig = px.scatter_mapbox(df,\n",
    "                        lat='V.GPSLAT.act.deg',lon='V.GPSLON.act.deg',color='V.SOG.act.kn',text='EntryDate',\n",
    "                        width=1000, height=600, \n",
    "                        title='observation period', \n",
    "                        labels={'V.GPSLAT.act.deg':'Latitude','V.GPSLON.act.deg':'Longitude','V.SOG.act.kn':'True Speed [kn]','EntryDate':'Date'},\n",
    "                        color_discrete_sequence=px.colors.qualitative.Safe, range_color=(0,df['V.SOG.act.kn'].max()))\n",
    "fig.update_layout(mapbox_style=\"open-street-map\",\n",
    "                  title_font_family=\"Arial\",\n",
    "                  title_font_color=\"grey\",\n",
    "                  title_font_size=24,\n",
    "                  title_x=0.5,\n",
    "                  legend=dict(title_font_family=\"Arial\",\n",
    "                                title_font_size=20,\n",
    "                                title_font_color=\"grey\",\n",
    "                                font=dict(family=\"Arial\",\n",
    "                                            size=18,\n",
    "                                            color=\"grey\"))\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vessel is operating between Europe and South America. Over the Atlantic the speed is high and rather constant. While entering the ports or ankering the speed is low. Therefore different passages types can be defined. A categorization is made as follows: long constant passages over the Atlantic, port entries with low vessel speed and all other passages. With this information a more precise EDA can be conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of all speed values\n",
    "px.histogram(df,x='V.SOG.act.kn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of speed on Atlantic passages\n",
    "px.histogram(df[(df['V.GPSLAT.act.deg']>=19) & (df['V.GPSLON.act.deg']<=-3)], x='V.SOG.act.kn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the lowest speed on Atlantic passages, 13.5kn are selected as threshold for low speed, i.e. port entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (df['V.GPSLAT.act.deg']>=19) & (df['V.GPSLON.act.deg']<=-3),\n",
    "    (df['V.GPSLAT.act.deg']<19) & (df['V.SOG.act.kn']>13.5),\n",
    "    (df['V.GPSLAT.act.deg']<19) & (df['V.SOG.act.kn']<=13.5),\n",
    "    (df['V.GPSLAT.act.deg']>-3) & (df['V.SOG.act.kn']>13.5),\n",
    "    (df['V.GPSLAT.act.deg']>-3) & (df['V.SOG.act.kn']<=13.5)]\n",
    "choices = ['Atlantic', 'SouthAmerica>13.5kn', 'SouthAmerica<13.5kn', 'Europe>13.5kn', 'Europe<13.5kn']\n",
    "df['passage_type'] = np.select(conditions, choices)\n",
    "df['passage_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split of the passage types is done by Latitude and speed. For the Latitude fixed values are used. These values are set with a sensible look at the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of high frequency data and daily performance trends report data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance trends report contains daily data observed by the crew. Especially trim data from this report seems to be more reliable than trim data from the sensor in the high frequency data. Both data sets should be joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from performance trend report\n",
    "df_daily = pd.read_csv('../data/PerformanceTrendsReport.csv',header=[0,1])\n",
    "df_daily.columns = df_daily.columns.map(lambda h: '  '.join(h).replace(' ', '_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has 45 columns and 1470 rows. The first entry is from 2017-02-18 thus the time series is longer than the one from the high frequency data. Not all of the features are important and some are related or just given in different units. Thus, only some of them are joined to the high frequency data (see list below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only important features in the data frame of the daily data\n",
    "df_daily = df_daily[['Report_Date__Date',\n",
    "                            'Report_Type__Noon/Autolog/Perf_test',\n",
    "                            'Speed_Observed__[kn]',\n",
    "                            'ME_Fuel_Cons__[t/24_h]',\n",
    "                            'ME_Power_(Propulsion)__[kW]',\n",
    "                            'ME_RPM__[rpm]',\n",
    "                            'Mean_Draft__[m]',\n",
    "                            'Trim__[m]',\n",
    "                            'Heading_Dir__[deg]',\n",
    "                            'True_Wind_Speed__[m/s]',\n",
    "                            'True_Wind_Dir__[deg]',\n",
    "                            'Wave_Height_[m]__[m]',\n",
    "                            'True_Wave_Dir__[deg]']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order keep the data frame clear, the column name are cleaned and the columns are marked with the postfix ```_daily```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column names\n",
    "df_daily = df_daily.rename(columns={'Report_Date__Date':'Date_daily',\n",
    "                            'Report_Type__Noon/Autolog/Perf_test':'Type_daily',\n",
    "                            'Speed_Observed__[kn]':'Speed_Obs_kn_daily',\n",
    "                            'ME_Fuel_Cons__[t/24_h]':'ME_Fuel_Cons_tP24h_daily',\n",
    "                            'ME_Power_(Propulsion)__[kW]':'ME_Power_Prop_kW_daily',\n",
    "                            'ME_RPM__[rpm]':'ME_RPM_rpm_daily',\n",
    "                            'Mean_Draft__[m]':'Mean_Draft_m_daily',\n",
    "                            'Trim__[m]':'Trim_m_daily',\n",
    "                            'Heading_Dir__[deg]':'Heading_Dir_deg_daily',\n",
    "                            'True_Wind_Speed__[m/s]':'True_Wind_Speed_mPs_daily',\n",
    "                            'True_Wind_Dir__[deg]':'True_Wind_Dir_deg_daily',\n",
    "                            'Wave_Height_[m]__[m]':'Wave_Height_m_daily',\n",
    "                            'True_Wave_Dir__[deg]':'True_Wave_Dir_deg_daily'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to join both data frames the date columns of both data frames need to be converted to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date columns to datetime\n",
    "df['EntryDate'] = pd.to_datetime(df['EntryDate'])\n",
    "df_daily['Date_daily'] = pd.to_datetime(df_daily['Date_daily'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['EntryDate'].min())\n",
    "print(df['EntryDate'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The daily data will be reduced to the time frame of the high frequency data which starts on 2021-05-31 and ends on 2021-10-31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce time frame of daily data‚\n",
    "df_daily = df_daily[df_daily['Date_daily']>='2021-04-30']\n",
    "df_daily = df_daily[df_daily['Date_daily']<='2021-11-01'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_daily['Date_daily'].min())\n",
    "print(df_daily['Date_daily'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the daily data starts on 2021-04-30 and ends on 2021-10-31. The next step is to add the daily data to the high frequency data as new features. Due to the different temporal resolutions, the same values of the daily data will be used for several consecutive timestamps of the high frequency data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to add new columns to high frequency data\n",
    "def add_daily_cols(name):\n",
    "    df[name] = np.nan\n",
    "    for index, row in df_daily.iterrows():\n",
    "        if index == 0:\n",
    "            end = row['Date_daily']\n",
    "            continue\n",
    "        else:\n",
    "            start = end\n",
    "            end = row['Date_daily']\n",
    "        df[name][(df['EntryDate']>=start) & (df['EntryDate']<end)]=row[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column names from daily data\n",
    "col_names = df_daily.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to high frequency data\n",
    "for i in col_names:\n",
    "    add_daily_cols(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the features from the daily data the total data frame now contains 57 features plus the time stamps of the high frequency data and the daily data and the correlation matrix can be checked again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Wind has NaN values, due to empty rows in the daily noon report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new feature to separate single passages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we only differentiated between passage type. In addition, it would be helpful to identify single trips from these passage types. Therefor a new feature ```trip_id``` is added. A new trip starts, when the passage type changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through row and create trip id\n",
    "trip_id = []\n",
    "for index, row in df.iterrows():\n",
    "    if index == 0:\n",
    "        t_id = 1\n",
    "        p_type = row['passage_type']\n",
    "    else:\n",
    "        if row['passage_type'] != p_type:\n",
    "            t_id +=1\n",
    "            p_type = row['passage_type']\n",
    "    trip_id.append(t_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join trip_id to dataframe\n",
    "df = pd.concat([df,pd.DataFrame({'trip_id':trip_id})],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('trip_id').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding feature of Power prediction\n",
    "Feature generated from a theoretical Engine Model (Matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load model of theoretical required Engine Power to move the vessel. Used as reference for the power used by the vessel while sailing.\n",
    "RandForestReg_EngineModel = '../models/RFReg_Engine_Model.sav'\n",
    "Engine_Model = pickle.load(open(RandForestReg_EngineModel, 'rb'))\n",
    "\n",
    "#generate input DataFrame for Engine Model. Input order: Draft [m], Trim [m], Speed [kn] \n",
    "df_inputEM = df[['Mean_Draft_m_daily', 'Trim_m_daily', 'Speed_Obs_kn_daily']]\n",
    "# Rename columns to column names used during fit of Engine Model\n",
    "df_inputEM = df_inputEM.rename(columns = {'Mean_Draft_m_daily': 'Mean_Draft_[m]', 'Trim_m_daily': 'Trim_[m]', 'Speed_Obs_kn_daily': 'Speed_[kn]'})\n",
    "\n",
    "# predict the required Power of the vessel with the help of the Engine Model\n",
    "Value = Engine_Model.predict(df_inputEM)\n",
    "\n",
    "# write a DataFrame for further use\n",
    "Power_predict = pd.DataFrame(data = Value, columns = {'Power_EM_predict'})\n",
    "\n",
    "# add Power prediction from Engine Model to DataFrame\n",
    "df = pd.concat([df, Power_predict], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft Marks\n",
    "1) Creating Rolling Average with window = 10.\n",
    "2) Calculate Trim, Heel, Draft  > DDM.TRIM.calc.m / DDM.HEEL.calc.m / DDM.DRAFT.calc.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy()\n",
    "\n",
    "# Create Rolling Average\n",
    "df_temp['DDM.FWDCL.10ava.m'] = df_temp['DDM.FWDCL.act.m'].rolling(window=10).mean()\n",
    "df_temp['DDM.AFTCL.10ava.m'] = df_temp['DDM.AFTCL.act.m'].rolling(window=10).mean()\n",
    "\n",
    "df_temp['DDM.MIDPS.10ava.m'] = df_temp['DDM.MIDPS.act.m'].rolling(window=10).mean()\n",
    "df_temp['DDM.MIDSB.10ava.m'] = df_temp['DDM.MIDSB.act.m'].rolling(window=10).mean()\n",
    "\n",
    "# Calculate TRIM, DRAFT, HEEL\n",
    "df_temp['DDM.TRIM.act.m'] = df_temp['DDM.FWDCL.10ava.m'] - df_temp['DDM.AFTCL.10ava.m']\n",
    "df_temp['DDM.DRAFT.act.m'] = (df_temp['DDM.MIDPS.10ava.m'] - df_temp['DDM.MIDSB.10ava.m'])/2\n",
    "df_temp['DDM.HEEL.act.m'] = df_temp['DDM.MIDPS.10ava.m'] - df_temp['DDM.MIDSB.10ava.m']\n",
    "\n",
    "# Fill first 10 values with backward filling\n",
    "df_temp['DDM.TRIM.act.m'].fillna(method='bfill', inplace=True)\n",
    "df_temp['DDM.DRAFT.act.m'].fillna(method='bfill', inplace=True)\n",
    "df_temp['DDM.HEEL.act.m'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "df_temp.drop(columns=['DDM.FWDCL.10ava.m', 'DDM.AFTCL.10ava.m', 'DDM.MIDPS.10ava.m', 'DDM.MIDSB.10ava.m'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign to Dataframe df\n",
    "df = df_temp.copy()\n",
    "del df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write .csv file for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/Featureselection03.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 01: Features dropped\n",
    "* 02: Daily date from Noon Report included as feature\n",
    "* 03: added Power prediction feature and rolling average"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff1a6782138c544d35c301dbb5edacda140ff557cdcf796e4240a41d30bcaa50"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
